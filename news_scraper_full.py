# -*- coding: utf-8 -*-
import os
import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import random

# ===========================================================
# è¨­å®š
# ===========================================================
DICTIONARY_JSON = "dictionary.json"
USE_OLLAMA = False  # Ollamaã‚’ä½¿ã†å ´åˆã¯Trueã«å¤‰æ›´
OLLAMA_MODEL = "llama3.2"

# ã‚¿ã‚°æŠ½å‡ºãƒ«ãƒ¼ãƒ«ï¼ˆé‡è¦ï¼‰
TAG_RULES = [
    ("æ–°è¡£è£…", ["æ–°è¡£è£…", "è¡£è£…", "ãŠæŠ«éœ²ç›®"]),
    ("ã‚³ãƒ©ãƒœ", ["ã‚³ãƒ©ãƒœ", "ã‚¿ã‚¤ã‚¢ãƒƒãƒ—", "ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"]),
    ("ç‚ä¸Š", ["ç‚ä¸Š", "ç‰©è­°", "æ‰¹åˆ¤", "è¬ç½ª", "ä¸ç¥¥äº‹"]),
    ("æµ·å¤–", ["æµ·å¤–", "overseas", "EN ", "global"]),
    ("é‡å¤§ç™ºè¡¨", ["é‡å¤§ç™ºè¡¨", "å’æ¥­", "å¼•é€€", "ç™ºè¡¨"]),
    ("ã‚¤ãƒ™ãƒ³ãƒˆ", ["ã‚¤ãƒ™ãƒ³ãƒˆ", "ãƒ©ã‚¤ãƒ–", "ãƒ•ã‚§ã‚¹"]),
]

# ===========================================================
# è¾æ›¸ãƒ­ãƒ¼ãƒ‰
# ===========================================================
try:
    with open(DICTIONARY_JSON, "r", encoding="utf-8") as f:
        DICT = json.load(f)
except FileNotFoundError:
    print(f"âŒ {DICTIONARY_JSON} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    exit(1)

QUERIES = DICT.get("queries", [])
KEYWORDS = DICT.get("keywords", [])
KINJI_COMMENTS = DICT.get("kinji_comments", {})
SETTINGS = DICT.get("settings", {})

NOTE_URL = SETTINGS.get("note_url", "")
LINE_URL = SETTINGS.get("line_url", "")
X_URL = SETTINGS.get("x_url", "")

# ===========================================================
# ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
# ===========================================================
def classify_by_keyword(title, snippet):
    """è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
    text = (title + " " + snippet).lower()
    
    for row in KEYWORDS:
        keyword = str(row.get("keyword", "")).lower()
        category = row.get("category", "")
        if keyword and keyword in text:
            return category
    
    return SETTINGS.get("default_category", "ãã®ä»–")

def category_to_class(category):
    """ã‚«ãƒ†ã‚´ãƒªåã‚’CSSã‚¯ãƒ©ã‚¹åã«å¤‰æ›"""
    return {
        "ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–": "cat-hololive",
        "ã«ã˜ã•ã‚“ã˜": "cat-nijisanji",
        "å€‹äººVTuber": "cat-indie",
        "ä¼æ¥­ã‚³ãƒ©ãƒœ": "cat-collab",
        "æµ·å¤–VTuber": "cat-global",
        "ãƒˆãƒ©ãƒ–ãƒ«ï¼ç‚ä¸Š": "cat-trouble",
        "ãã®ä»–": "cat-none",
    }.get(category, "cat-none")

# ===========================================================
# ã‚¿ã‚°æŠ½å‡ºï¼ˆé‡è¦æ©Ÿèƒ½ï¼‰
# ===========================================================
def extract_tags(title, snippet):
    """è¨˜äº‹ã‹ã‚‰ã‚¿ã‚°ã‚’æŠ½å‡ºï¼ˆæœ€å¤§3å€‹ï¼‰"""
    text = (title + " " + snippet).lower()
    tags = []
    
    for label, keywords in TAG_RULES:
        for keyword in keywords:
            if keyword.lower() in text:
                tags.append(label)
                break  # åŒã˜ã‚¿ã‚°ã¯1å›ã ã‘
    
    # é‡è¤‡ã‚’é™¤å»ã—ã¦æœ€å¤§3å€‹ã¾ã§
    return list(dict.fromkeys(tags))[:3]

# ===========================================================
# é‡‘æ¬¡ã‚³ãƒ¡ãƒ³ãƒˆ
# ===========================================================
_used_comments = {}

def pick_unique_comment(category):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªé‡‘æ¬¡ã‚³ãƒ¡ãƒ³ãƒˆã‚’é¸æŠ"""
    if category not in KINJI_COMMENTS:
        category = "ãã®ä»–"
    
    comments = [c.get("comment_text", "") for c in KINJI_COMMENTS.get(category, [])]
    
    if not comments:
        return ""
    
    used = _used_comments.setdefault(category, set())
    remain = [c for c in comments if c not in used]
    
    if not remain:
        used.clear()
        remain = comments[:]
    
    chosen = random.choice(remain)
    used.add(chosen)
    return chosen

# ===========================================================
# ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
# ===========================================================
def fetch_all_news():
    """è¾æ›¸ã®ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    print("â–¶ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚’é–‹å§‹...")
    headers = {"User-Agent": "Mozilla/5.0"}
    all_articles = []
    
    for q in QUERIES:
        if not q.get("enabled", False):
            continue
        
        search_query = q.get("search_query", "").strip()
        max_items = int(q.get("max_items", 3))
        
        print(f"  â†’ {search_query} ã‚’å–å¾—ä¸­...")
        
        url = f"https://www.bing.com/news/search?q={search_query}&format=rss"
        
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "xml")
            
            items = soup.find_all("item")[:max_items]
            
            for item in items:
                article = {
                    "title": item.title.text if item.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—",
                    "url": item.link.text if item.link else "#",
                    "snippet": item.description.text if item.description else "èª¬æ˜ãªã—",
                    "date": item.pubDate.text if item.pubDate else datetime.now().strftime("%Y-%m-%d"),
                }
                all_articles.append(article)
        
        except Exception as e:
            print(f"âš  {search_query} ã®å–å¾—å¤±æ•—:", e)
            continue
    
    print(f"  â†’ åˆè¨ˆ {len(all_articles)} ä»¶å–å¾—")
    return all_articles

# ===========================================================
# é‡è¤‡é™¤å»
# ===========================================================
def dedupe_articles(articles):
    """ã‚¿ã‚¤ãƒˆãƒ«ã§é‡è¤‡ã‚’é™¤å»"""
    seen = set()
    deduped = []
    for a in articles:
        key = a["title"]
        if key not in seen:
            seen.add(key)
            deduped.append(a)
    print(f"  â†’ {len(deduped)} ä»¶ã«é‡è¤‡é™¤å»å®Œäº†")
    return deduped

# ===========================================================
# Ollama AIåˆ†æï¼ˆPage2ç”¨ï¼‰
# ===========================================================
def analyze_with_ollama_deep(articles):
    """TOP2è¨˜äº‹ã‚’Ollamaã§æ·±æ˜ã‚Šåˆ†æ"""
    if not USE_OLLAMA or len(articles) < 2:
        return None
    
    top2 = articles[:2]
    
    prompt = f"""ä»¥ä¸‹ã®VTuberãƒ‹ãƒ¥ãƒ¼ã‚¹2ä»¶ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹1ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {top2[0]['title']}
å†…å®¹: {top2[0]['snippet']}

ã€è¨˜äº‹2ã€‘
ã‚¿ã‚¤ãƒˆãƒ«: {top2[1]['title']}
å†…å®¹: {top2[1]['snippet']}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

â–  è¦ç‚¹3è¡Œ
ãƒ»
ãƒ»
ãƒ»

â–  èƒŒæ™¯èª¬æ˜
ï¼ˆãªãœã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒé‡è¦ã‹ï¼‰

â–  æ–‡è„ˆè§£é‡ˆ
ï¼ˆVTuberæ¥­ç•Œå…¨ä½“ã¸ã®å½±éŸ¿ï¼‰

â–  ä»Šæ—¥ã®å‚¾å‘
ï¼ˆæœ¬æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨ä½“ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰

â–  XæŠ•ç¨¿æ¡ˆ
ï¼ˆ140å­—ä»¥å†…ã§æŠ•ç¨¿ã§ãã‚‹æ–‡ç« ï¼‰"""
    
    try:
        print("  â†’ Ollamaåˆ†æä¸­...")
        r = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        data = r.json()
        return data.get("response", "")
    except Exception as e:
        print(f"âš  Ollamaåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ===========================================================
# JSONä¿å­˜
# ===========================================================
def save_to_json(articles, date_str):
    """ãƒ‡ãƒ¼ã‚¿ã‚’JSONã§ä¿å­˜"""
    archive_dir = "archive/data"
    os.makedirs(archive_dir, exist_ok=True)
    
    # ã‚«ãƒ†ã‚´ãƒªã¨ã‚¿ã‚°ã®é›†è¨ˆ
    categories = {}
    tags = {}
    
    for a in articles:
        cat = a.get("category", "ãã®ä»–")
        categories[cat] = categories.get(cat, 0) + 1
        
        for tag in a.get("tags", []):
            tags[tag] = tags.get(tag, 0) + 1
    
    data = {
        "date": date_str,
        "articles": articles,
        "article_count": len(articles),
        "categories": categories,
        "tags": tags
    }
    
    filepath = os.path.join(archive_dir, f"news_{date_str}.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {filepath}")

# ===========================================================
# Page1ç”Ÿæˆï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ï¼‰
# ===========================================================
def build_page1(articles, date_str):
    """Page1: ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    
    # ã‚«ãƒ¼ãƒ‰ç”Ÿæˆï¼ˆé‡è¦ï¼šdata-category ã¨ data-tags ã‚’åŸ‹ã‚è¾¼ã‚€ï¼‰
    cards_html = ""
    for a in articles:
        category = a.get("category", "ãã®ä»–")
        tags = a.get("tags", [])
        class_name = category_to_class(category)
        
        snippet = a['snippet'].replace('<', '&lt;').replace('>', '&gt;')
        if len(snippet) > 150:
            snippet = snippet[:150] + "..."
        
        # ã‚¿ã‚°ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§
        tags_str = ",".join(tags) if tags else ""
        
        # ã‚¿ã‚°ãƒãƒƒãƒ—HTML
        tags_html = ""
        for tag in tags:
            tags_html += f'<span class="tag-chip">{tag}</span>'
        
        # Xå…±æœ‰ãƒœã‚¿ãƒ³
        share_text = f"{a['title']} {a['url']}"
        share_url = f"https://twitter.com/intent/tweet?text={requests.utils.quote(share_text)}"
        
        kinji_comment = pick_unique_comment(category)
        
        cards_html += f'''      <article class="card {class_name}"
               data-category="{category}"
               data-tags="{tags_str}">
        <span class="category">{category}</span>
        <h3>{a['title']}</h3>
        <p class="snippet">{snippet}</p>
        <div class="tags-container">
{tags_html}
        </div>'''
        
        if kinji_comment:
            cards_html += f'''
        <div class="kinji-comment">{kinji_comment}</div>'''
        
        cards_html += f'''
        <div class="card-footer">
          <a href="{a['url']}" target="_blank">è¨˜äº‹ã‚’èª­ã‚€ â†’</a>
          <a href="{share_url}" target="_blank" class="share-x">Xã§å…±æœ‰</a>
        </div>
        <span class="date">{a['date']}</span>
      </article>
'''
    
    # å…¨ã‚«ãƒ†ã‚´ãƒªã¨ã‚¿ã‚°ã‚’æŠ½å‡º
    all_categories = sorted(list(set([a.get("category", "ãã®ä»–") for a in articles])))
    all_tags = sorted(list(set([tag for a in articles for tag in a.get("tags", [])])))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚¿ãƒ–HTML
    category_tabs = '<button class="tab-btn active" data-filter="all">ã™ã¹ã¦</button>\n'
    for cat in all_categories:
        category_tabs += f'        <button class="tab-btn" data-filter="{cat}">{cat}</button>\n'
    
    # ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿HTML
    tag_filters = ""
    for tag in all_tags:
        tag_filters += f'        <button class="filter-btn" data-tag="{tag}">{tag}</button>\n'
    
    html = f'''<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>æœ¬æ—¥ã®å‚™å¿˜éŒ² â€” {date_str} | {SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</title>
  <link rel="stylesheet" href="style.css">
  <style>
    /* ã‚¿ãƒ–ã¨ãƒ•ã‚£ãƒ«ã‚¿ */
    .tabs {{
      display: flex;
      gap: 8px;
      margin-bottom: 16px;
      flex-wrap: wrap;
    }}
    .tab-btn, .filter-btn {{
      padding: 8px 16px;
      border: 1px solid #D1D5DB;
      background: #F9FAFB;
      border-radius: 20px;
      cursor: pointer;
      font-size: 0.9rem;
      transition: all 0.2s;
    }}
    .tab-btn:hover, .filter-btn:hover {{
      background: #E5E7EB;
    }}
    .tab-btn.active {{
      background: #C7463C;
      color: white;
      border-color: #C7463C;
    }}
    .filter-btn.active {{
      background: #D6B86A;
      color: white;
      border-color: #D6B86A;
    }}
    /* ã‚¿ã‚°ãƒãƒƒãƒ— */
    .tags-container {{
      display: flex;
      gap: 6px;
      margin: 8px 0;
      flex-wrap: wrap;
    }}
    .tag-chip {{
      display: inline-block;
      padding: 4px 10px;
      background: rgba(214, 184, 106, 0.15);
      color: #D6B86A;
      border-radius: 12px;
      font-size: 0.75rem;
      font-weight: 600;
    }}
    /* ã‚«ãƒ¼ãƒ‰ãƒ•ãƒƒã‚¿ãƒ¼ */
    .card-footer {{
      display: flex;
      gap: 12px;
      margin-top: 8px;
    }}
    .share-x {{
      color: #1DA1F2;
      font-size: 0.85rem;
      font-weight: 600;
    }}
    /* ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºåˆ¶å¾¡ */
    .card.hidden {{
      display: none;
    }}
  </style>
</head>
<body class="page-news">

  <header class="site-header">
    <div class="site-header-inner">
      <div class="site-title-group">
        <h1 class="logo-main">{SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</h1>
        <p class="logo-sub">{SETTINGS.get("site_subtitle", "å‚™å¿˜éŒ²")}</p>
      </div>
      <nav class="site-nav">
        <a href="index.html" class="nav-link">ãƒˆãƒƒãƒ—</a>
        <a href="page2_{date_str}.html" class="nav-link">AIæ·±æ˜ã‚Š</a>
        <a href="archive/index.html" class="nav-link">éå»ã®è¨˜éŒ²</a>
      </nav>
    </div>
  </header>

  <main class="news-main">
    <div class="page-heading">
      <h2 class="page-title">æœ¬æ—¥ã®å‚™å¿˜éŒ² â€” {date_str}</h2>
      <p class="page-intro">VTuberæ¥­ç•Œã®å‹•ãã‚’è¨˜éŒ²ã€‚æ—¥ã€…ã®ç³§ã¨ã›ã‚ˆã€‚</p>
    </div>

    <!-- ã‚«ãƒ†ã‚´ãƒªã‚¿ãƒ– -->
    <div class="tabs">
{category_tabs}
    </div>

    <!-- ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ -->
    <div class="tabs" style="margin-top: 8px;">
      <span style="font-size: 0.9rem; color: #6B7280; align-self: center;">ã‚¿ã‚°ï¼š</span>
{tag_filters}
    </div>

    <section class="news-section">
      <div class="cards-container">
{cards_html}
      </div>
    </section>

    <section class="news-section">
      <h3 class="section-title">æ›´ãªã‚‹å­¦ã³ã¸</h3>
      <div class="callout note-callout">
        <span class="callout-title">ğŸ“ noteã§è©³ã—ãå­¦ã¶</span>
        <p>é‡‘æ¬¡ã®æˆ¦ç•¥è«–ãƒ»æ·±æ˜ã‚Šåˆ†æã‚’noteã§å…¬é–‹ä¸­ã€‚</p>
        <a href="{NOTE_URL}" target="_blank" class="callout-link">noteã‚’èª­ã‚€ â†’</a>
      </div>
      <div class="callout line-callout" style="margin-top:12px;">
        <span class="callout-title">ğŸ’¬ å…¬å¼LINEã§ç›¸è«‡</span>
        <p>å€‹åˆ¥ç›¸è«‡ãƒ»æˆ¦ç•¥ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¯LINEã«ã¦ã€‚</p>
        <a href="{LINE_URL}" target="_blank" class="callout-link">LINEã‚’è¿½åŠ  â†’</a>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <p>&copy; 2024 {SETTINGS.get("author_name", "é‡‘æ¬¡")} | VTuberå‚™å¿˜éŒ²</p>
  </footer>

  <script>
    // ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
    const tabBtns = document.querySelectorAll('.tab-btn');
    const cards = document.querySelectorAll('.card');
    
    tabBtns.forEach(btn => {{
      btn.addEventListener('click', () => {{
        // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹åˆ‡ã‚Šæ›¿ãˆ
        tabBtns.forEach(b => b.classList.remove('active'));
        btn.classList.add('active');
        
        const filter = btn.dataset.filter;
        
        cards.forEach(card => {{
          if (filter === 'all' || card.dataset.category === filter) {{
            card.classList.remove('hidden');
          }} else {{
            card.classList.add('hidden');
          }}
        }});
      }});
    }});
    
    // ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿
    const filterBtns = document.querySelectorAll('.filter-btn');
    
    filterBtns.forEach(btn => {{
      btn.addEventListener('click', () => {{
        btn.classList.toggle('active');
        
        // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¿ã‚°ã‚’å–å¾—
        const activeTags = Array.from(filterBtns)
          .filter(b => b.classList.contains('active'))
          .map(b => b.dataset.tag);
        
        cards.forEach(card => {{
          const cardTags = card.dataset.tags ? card.dataset.tags.split(',') : [];
          
          if (activeTags.length === 0) {{
            // ã‚¿ã‚°é¸æŠãªã— = ã™ã¹ã¦è¡¨ç¤º
            card.classList.remove('hidden');
          }} else {{
            // é¸æŠã•ã‚ŒãŸã‚¿ã‚°ã®ã„ãšã‚Œã‹ã‚’å«ã‚€ã‹
            const hasTag = activeTags.some(tag => cardTags.includes(tag));
            if (hasTag) {{
              card.classList.remove('hidden');
            }} else {{
              card.classList.add('hidden');
            }}
          }}
        }});
      }});
    }});
  </script>

</body>
</html>'''
    
    filename = f"news_{date_str}.html"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html)
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ã‚‚ã‚³ãƒ”ãƒ¼
    archive_dir = "archive"
    os.makedirs(archive_dir, exist_ok=True)
    with open(f"{archive_dir}/{filename}", "w", encoding="utf-8") as f:
        f.write(html)
    
    print(f"âœ“ Page1ç”Ÿæˆ: {filename}")
    return filename

# ===========================================================
# Page2ç”Ÿæˆï¼ˆAIæ·±æ˜ã‚Šï¼‰
# ===========================================================
def build_page2(articles, ai_analysis, date_str):
    """Page2: AIæ·±æ˜ã‚Šãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    
    if not ai_analysis:
        ai_analysis = "â€» AIåˆ†æã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
    
    # AIåˆ†æã‚’HTMLã«å¤‰æ›
    analysis_html = ai_analysis.replace("\n", "<br>")
    
    html = f'''<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AIæ·±æ˜ã‚Šåˆ†æ â€” {date_str} | {SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</title>
  <link rel="stylesheet" href="style.css">
</head>
<body class="page-news">

  <header class="site-header">
    <div class="site-header-inner">
      <div class="site-title-group">
        <h1 class="logo-main">{SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</h1>
        <p class="logo-sub">AIæ·±æ˜ã‚Šåˆ†æ</p>
      </div>
      <nav class="site-nav">
        <a href="index.html" class="nav-link">ãƒˆãƒƒãƒ—</a>
        <a href="news_{date_str}.html" class="nav-link">ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§</a>
        <a href="archive/index.html" class="nav-link">éå»ã®è¨˜éŒ²</a>
      </nav>
    </div>
  </header>

  <main class="news-main">
    <div class="page-heading">
      <h2 class="page-title">AIæ·±æ˜ã‚Šåˆ†æ â€” {date_str}</h2>
      <p class="page-intro">æœ¬æ—¥ã®æ³¨ç›®è¨˜äº‹ã‚’AIãŒæ·±æ˜ã‚Šåˆ†æã€‚</p>
    </div>

    <section class="news-section">
      <div class="bamc-block" style="line-height: 1.8;">
{analysis_html}
      </div>
    </section>

    <section class="news-section">
      <a href="news_{date_str}.html" class="btn-primary">â† ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã«æˆ»ã‚‹</a>
    </section>
  </main>

  <footer class="site-footer">
    <p>&copy; 2024 {SETTINGS.get("author_name", "é‡‘æ¬¡")} | VTuberå‚™å¿˜éŒ²</p>
  </footer>

</body>
</html>'''
    
    filename = f"page2_{date_str}.html"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html)
    
    print(f"âœ“ Page2ç”Ÿæˆ: {filename}")
    return filename

# ===========================================================
# ãƒãƒ¼ã‚¿ãƒ«ã¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
# ===========================================================
def create_portal_page(latest_file):
    """index.htmlã‚’ç”Ÿæˆ"""
    html = f'''<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>{SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</title>
  <link rel="stylesheet" href="style.css">
</head>
<body class="page-portal">

  <header class="site-header">
    <div class="site-header-inner">
      <div class="site-title-group">
        <h1 class="logo-main">{SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</h1>
        <p class="logo-sub">{SETTINGS.get("site_subtitle", "å‚™å¿˜éŒ²")}</p>
        <p class="logo-tagline">{SETTINGS.get("site_tagline", "æ˜æ—¥ã‚’æ‹“ãè€…ã¸ã®é“æ¨™")}</p>
      </div>
    </div>
  </header>

  <main class="portal-main">
    <div class="hero">
      <h2 class="hero-lead">VTuberæ¥­ç•Œã®æ—¥ã€…ã‚’è¨˜éŒ²ã—ã€é“ã‚’ç…§ã‚‰ã™ã€‚</h2>
      <p class="hero-text">
        é‡‘æ¬¡ãŒæ¯æ—¥VTuberãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ãƒ»åˆ†æã€‚<br>
        ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ã¨AIæ·±æ˜ã‚Šã§ã€å€‹äººå‹¢VTuberã®æˆé•·ã‚’æ”¯æ´ã™ã‚‹ã€‚
      </p>
      <div class="hero-actions">
        <a href="{latest_file}" class="btn-primary">æœ¬æ—¥ã®å‚™å¿˜éŒ²ã‚’è¦‹ã‚‹</a>
        <a href="archive/index.html" class="btn-secondary">éå»ã®è¨˜éŒ²</a>
      </div>
    </div>
  </main>

  <footer class="site-footer">
    <p>&copy; 2024 {SETTINGS.get("author_name", "é‡‘æ¬¡")} | VTuberå‚™å¿˜éŒ²</p>
  </footer>

</body>
</html>'''
    
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html)
    
    print("âœ“ ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ä½œæˆ: index.html")

def create_archive_index():
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    archive_dir = "archive"
    files = sorted([f for f in os.listdir(archive_dir) if f.startswith("news_") and f.endswith(".html")], reverse=True)
    
    html = f'''<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <title>éå»ã®è¨˜éŒ² | {SETTINGS.get("site_title", "é‡‘æ¬¡ã®å¯ºå­å±‹")}</title>
  <link rel="stylesheet" href="../style.css">
</head>
<body>
  <header class="site-header">
    <div class="site-header-inner">
      <h1 class="logo-main">éå»ã®å‚™å¿˜éŒ²</h1>
      <nav class="site-nav">
        <a href="../index.html" class="nav-link">ãƒˆãƒƒãƒ—</a>
      </nav>
    </div>
  </header>

  <main class="archive-container">
    <div class="archive-list">
'''
    
    for filename in files:
        date_str = filename.replace("news_", "").replace(".html", "")
        html += f'      <div class="archive-item"><a href="{filename}"><span>{date_str} ã®è¨˜éŒ²</span><span class="archive-arrow">â†’</span></a></div>\n'
    
    html += '''    </div>
  </main>
</body>
</html>'''
    
    with open(f"{archive_dir}/index.html", "w", encoding="utf-8") as f:
        f.write(html)
    
    print("âœ“ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")

# ===========================================================
# ãƒ¡ã‚¤ãƒ³
# ===========================================================
def main():
    print("\n========== VTuberãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆå®Œå…¨ç‰ˆç”Ÿæˆ ==========")
    
    # â‘  ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
    articles_all = fetch_all_news()
    articles = dedupe_articles(articles_all)
    
    if not articles:
        print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # â‘¡ ã‚«ãƒ†ã‚´ãƒªã¨ã‚¿ã‚°ã‚’ä»˜ä¸
    print("\nâ–¶ ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¿ã‚°åˆ†æä¸­...")
    for a in articles:
        a["category"] = classify_by_keyword(a["title"], a["snippet"])
        a["tags"] = extract_tags(a["title"], a["snippet"])
    
    print(f"âœ“ {len(articles)}ä»¶ã®è¨˜äº‹ã‚’åˆ†æå®Œäº†")
    
    # â‘¢ JSONä¿å­˜
    date_str = datetime.today().strftime("%Y-%m-%d")
    save_to_json(articles, date_str)
    
    # â‘£ Page1ç”Ÿæˆ
    print("\nâ–¶ Page1ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ï¼‰ç”Ÿæˆä¸­...")
    page1_file = build_page1(articles, date_str)
    
    # â‘¤ Page2ç”Ÿæˆï¼ˆAIæ·±æ˜ã‚Šï¼‰
    print("\nâ–¶ Page2ï¼ˆAIæ·±æ˜ã‚Šï¼‰ç”Ÿæˆä¸­...")
    ai_analysis = analyze_with_ollama_deep(articles) if USE_OLLAMA else None
    page2_file = build_page2(articles, ai_analysis, date_str)
    
    # â‘¥ ãƒãƒ¼ã‚¿ãƒ«ã¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
    create_portal_page(page1_file)
    create_archive_index()
    
    print("\n" + "=" * 50)
    print(f"âœ… ç”Ÿæˆå®Œäº†")
    print(f"  Page1: {page1_file}")
    print(f"  Page2: {page2_file}")
    print("=" * 50)

if __name__ == "__main__":
    main()
